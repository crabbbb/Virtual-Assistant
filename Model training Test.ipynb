{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e736f86",
   "metadata": {},
   "source": [
    "### Resource use \n",
    "\n",
    "- [Reference Video](https://www.youtube.com/watch?v=u2diEa4VT4M&t=83s&ab_channel=AllAboutAI)\n",
    "- [Run Llama 2 Locally with Python](https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/)\n",
    "- [llama-cpp-python](https://pypi.org/project/llama-cpp-python/)\n",
    "    - Tutorial \n",
    "        - https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- [Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09dcb571",
   "metadata": {},
   "source": [
    "## Model Installing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c8201df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: llama_cpp_python\n",
      "Version: 0.2.64\n",
      "Summary: Python bindings for the llama.cpp library\n",
      "Home-page: \n",
      "Author: \n",
      "Author-email: Andrei Betlen <abetlen@gmail.com>\n",
      "License: MIT\n",
      "Location: C:\\Users\\LENOVO\\anaconda3\\Lib\\site-packages\n",
      "Requires: diskcache, jinja2, numpy, typing-extensions\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# pip install llama-cpp-python\n",
    "\n",
    "# version check \n",
    "pip show llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319055f3",
   "metadata": {},
   "source": [
    "### Test Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf4ff2ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from model/mistral-7b-instruct-v0.1.Q5_K_M.gguf (version GGUF V2)\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = mistralai_mistral-7b-instruct-v0.1\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 17\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
      "llm_load_print_meta: format           = GGUF V2\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32000\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 32768\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 14336\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 32768\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = Q5_K - Medium\n",
      "llm_load_print_meta: model params     = 7.24 B\n",
      "llm_load_print_meta: model size       = 4.78 GiB (5.67 BPW) \n",
      "llm_load_print_meta: general.name     = mistralai_mistral-7b-instruct-v0.1\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4892.99 MiB\n",
      "...................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    64.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   64.00 MiB, K (f16):   32.00 MiB, V (f16):   32.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =    81.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.name': 'mistralai_mistral-7b-instruct-v0.1', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '2', 'general.file_type': '17', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0'}\n",
      "Using fallback chat format: None\n",
      "\n",
      "llama_print_timings:        load time =   11899.01 ms\n",
      "llama_print_timings:      sample time =       3.51 ms /    16 runs   (    0.22 ms per token,  4561.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11898.45 ms /    16 tokens (  743.65 ms per token,     1.34 tokens per second)\n",
      "llama_print_timings:        eval time =    6031.73 ms /    15 runs   (  402.12 ms per token,     2.49 tokens per second)\n",
      "llama_print_timings:       total time =   18028.74 ms /    31 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday B: \n"
     ]
    }
   ],
   "source": [
    "# load the large language model file\n",
    "from llama_cpp import Llama\n",
    "LLM = Llama(model_path=\"model/mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "\n",
    "# create a text prompt\n",
    "prompt = \"Q: What are the names of the days of the week? A:\"\n",
    "\n",
    "# generate a response (takes several seconds)\n",
    "output = LLM(prompt)\n",
    "\n",
    "# display the response\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71f4e501",
   "metadata": {},
   "source": [
    "## Train Model - Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4e99fac",
   "metadata": {},
   "source": [
    "[ShortCut Key](https://digitalhumanities.hkust.edu.hk/tutorials/jupyter-notebook-tips-and-shortcuts/)\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Reference \n",
    "- [Guide to Fine-Tuning LLMs](https://www.datacamp.com/tutorial/fine-tuning-large-language-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bae063",
   "metadata": {},
   "source": [
    "### Read data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baef29ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install trl \n",
    "# pip install peft\n",
    "# pip install torch\n",
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e6d094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Package(s) not found: torch\n"
     ]
    }
   ],
   "source": [
    "# pip show torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aec66ce",
   "metadata": {},
   "source": [
    "Error facing when installing trl, peft & torch\n",
    "\n",
    "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus\n",
    "    we cannot accurately determine which files belong to it which would lead to\n",
    "    only a partial uninstall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d86f416",
   "metadata": {},
   "source": [
    "[Pandas Official DataFrame Tutorial](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
    "<br><br>\n",
    "[W3School Pandas DataFrame](https://www.w3schools.com/python/pandas/pandas_dataframes.asp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "12d5bb6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (1.24.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38727f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset json (C:/Users/LENOVO/.cache/huggingface/datasets/mteb___json/mteb--tweet_sentiment_extraction-19a608df77d581ad/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30943c225fb24aa1aaed3a2ddcd08632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['id', 'text', 'label', 'label_text'],\n",
      "        num_rows: 27481\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['id', 'text', 'label', 'label_text'],\n",
      "        num_rows: 3534\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "104a2561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27476</th>\n",
       "      <td>4eac33d1c0</td>\n",
       "      <td>wish we could come see u on Denver  husband l...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27477</th>\n",
       "      <td>4f4c4fc327</td>\n",
       "      <td>I`ve wondered about rake to.  The client has ...</td>\n",
       "      <td>0</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27478</th>\n",
       "      <td>f67aae2310</td>\n",
       "      <td>Yay good for both of you. Enjoy the break - y...</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27479</th>\n",
       "      <td>ed167662a5</td>\n",
       "      <td>But it was worth it  ****.</td>\n",
       "      <td>2</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27480</th>\n",
       "      <td>6f7127d9d7</td>\n",
       "      <td>All this flirting going on - The ATG smiles...</td>\n",
       "      <td>1</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27481 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id                                               text  label  \\\n",
       "0      cb774db0d1                I`d have responded, if I were going      1   \n",
       "1      549e992a42      Sooo SAD I will miss you here in San Diego!!!      0   \n",
       "2      088c60f138                          my boss is bullying me...      0   \n",
       "3      9642c003ef                     what interview! leave me alone      0   \n",
       "4      358bd9e861   Sons of ****, why couldn`t they put them on t...      0   \n",
       "...           ...                                                ...    ...   \n",
       "27476  4eac33d1c0   wish we could come see u on Denver  husband l...      0   \n",
       "27477  4f4c4fc327   I`ve wondered about rake to.  The client has ...      0   \n",
       "27478  f67aae2310   Yay good for both of you. Enjoy the break - y...      2   \n",
       "27479  ed167662a5                         But it was worth it  ****.      2   \n",
       "27480  6f7127d9d7     All this flirting going on - The ATG smiles...      1   \n",
       "\n",
       "      label_text  \n",
       "0        neutral  \n",
       "1       negative  \n",
       "2       negative  \n",
       "3       negative  \n",
       "4       negative  \n",
       "...          ...  \n",
       "27476   negative  \n",
       "27477   negative  \n",
       "27478   positive  \n",
       "27479   positive  \n",
       "27480    neutral  \n",
       "\n",
       "[27481 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show data in table form\n",
    "df = pd.DataFrame(dataset['train'])\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e555ceab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109924"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# directly convert to pandas format without create library  \n",
    "pandas_format = dataset[\"train\"].to_pandas()\n",
    "display(pandas_format.size) # check size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1b67db",
   "metadata": {},
   "source": [
    "<code>display()</code> can directly display in table form ( more better ) <br>\n",
    "- <code>DataFrame.head()</code> first 5 row <br>\n",
    "- <code>DataFrame.tail()</code> last 5 row <br>\n",
    "\n",
    "<code>print()</code> will only display without formating <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41498ac",
   "metadata": {},
   "source": [
    "# Fucking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9fadecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: huggingface_hub in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.15.1)\n",
      "Requirement already satisfied: filelock in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub) (3.9.0)\n",
      "Requirement already satisfied: fsspec in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub) (2023.4.0)\n",
      "Requirement already satisfied: requests in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.65.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub) (6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub) (4.7.1)\n",
      "Requirement already satisfied: packaging>=20.9 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from huggingface_hub) (23.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tqdm>=4.42.1->huggingface_hub) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests->huggingface_hub) (2024.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "deb3b64f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f59d1dffc14e7cbea9b214d0a88430",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)openorca.Q4_K_M.gguf:   0%|          | 0.00/4.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OSError",
     "evalue": "Consistency check failed: file should be of size 4368450304 but has size 2613202935 ((…)openorca.Q4_K_M.gguf).\nWe are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTheBloke/Mistral-7B-OpenOrca-GGUF\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m model_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistral-7b-openorca.Q4_K_M.gguf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;66;03m# this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m model_path \u001b[38;5;241m=\u001b[39m hf_hub_download(model_name, filename\u001b[38;5;241m=\u001b[39mmodel_file)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# model_path = \"model\\mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m## Instantiate model from downloaded file\u001b[39;00m\n\u001b[0;32m     13\u001b[0m llm \u001b[38;5;241m=\u001b[39m Llama(\n\u001b[0;32m     14\u001b[0m     model_path\u001b[38;5;241m=\u001b[39mmodel_path,\n\u001b[0;32m     15\u001b[0m     n_ctx\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16000\u001b[39m,  \u001b[38;5;66;03m# Context length to use\u001b[39;00m\n\u001b[0;32m     16\u001b[0m     n_threads\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,            \u001b[38;5;66;03m# Number of CPU threads to use\u001b[39;00m\n\u001b[0;32m     17\u001b[0m     n_gpu_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m        \u001b[38;5;66;03m# Number of model layers to offload to GPU\u001b[39;00m\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\utils\\_validators.py:118\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    116\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 118\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:1364\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, local_dir_use_symlinks, user_agent, force_download, force_filename, proxies, etag_timeout, resume_download, token, local_files_only, legacy_cache_layout)\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m temp_file_manager() \u001b[38;5;28;01mas\u001b[39;00m temp_file:\n\u001b[0;32m   1362\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownloading \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, temp_file\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m-> 1364\u001b[0m     http_get(\n\u001b[0;32m   1365\u001b[0m         url_to_download,\n\u001b[0;32m   1366\u001b[0m         temp_file,\n\u001b[0;32m   1367\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[0;32m   1368\u001b[0m         resume_size\u001b[38;5;241m=\u001b[39mresume_size,\n\u001b[0;32m   1369\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[0;32m   1370\u001b[0m         expected_size\u001b[38;5;241m=\u001b[39mexpected_size,\n\u001b[0;32m   1371\u001b[0m     )\n\u001b[0;32m   1373\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m local_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1374\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStoring \u001b[39m\u001b[38;5;132;01m{\u001b[39;00murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in cache at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:547\u001b[0m, in \u001b[0;36mhttp_get\u001b[1;34m(url, temp_file, proxies, resume_size, headers, timeout, max_retries, expected_size)\u001b[0m\n\u001b[0;32m    544\u001b[0m         temp_file\u001b[38;5;241m.\u001b[39mwrite(chunk)\n\u001b[0;32m    546\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m expected_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m expected_size \u001b[38;5;241m!=\u001b[39m temp_file\u001b[38;5;241m.\u001b[39mtell():\n\u001b[1;32m--> 547\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    548\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsistency check failed: file should be of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexpected_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m but has size\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    549\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemp_file\u001b[38;5;241m.\u001b[39mtell()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdisplayed_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mWe are sorry for the inconvenience. Please retry download and\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    550\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m pass `force_download=True, resume_download=False` as argument.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf the issue persists, please let us\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m know by opening an issue on https://github.com/huggingface/huggingface_hub.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    552\u001b[0m     )\n\u001b[0;32m    554\u001b[0m progress\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mOSError\u001b[0m: Consistency check failed: file should be of size 4368450304 but has size 2613202935 ((…)openorca.Q4_K_M.gguf).\nWe are sorry for the inconvenience. Please retry download and pass `force_download=True, resume_download=False` as argument.\nIf the issue persists, please let us know by opening an issue on https://github.com/huggingface/huggingface_hub."
     ]
    }
   ],
   "source": [
    "## Imports\n",
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama\n",
    "\n",
    "## Download the GGUF model\n",
    "model_name = \"TheBloke/Mistral-7B-OpenOrca-GGUF\"\n",
    "model_file = \"mistral-7b-openorca.Q4_K_M.gguf\" # this is the specific model file we'll use in this example. It's a 4-bit quant, but other levels of quantization are available in the model repo if preferred\n",
    "model_path = hf_hub_download(model_name, filename=model_file)\n",
    "\n",
    "# model_path = \"model\\mistral-7b-instruct-v0.1.Q5_K_M.gguf\"\n",
    "\n",
    "## Instantiate model from downloaded file\n",
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_ctx=16000,  # Context length to use\n",
    "    n_threads=8,            # Number of CPU threads to use\n",
    "    n_gpu_layers=1        # Number of model layers to offload to GPU\n",
    ")\n",
    "\n",
    "## Generation kwargs\n",
    "generation_kwargs = {\n",
    "    \"max_tokens\":2000,\n",
    "    \"stop\":[\"</s>\"],\n",
    "    \"echo\":False, # Echo the prompt in the output\n",
    "    \"top_k\":1 # This is essentially greedy decoding, since the model will always return the highest-probability token. Set this value > 1 for sampling decoding\n",
    "}\n",
    "\n",
    "## Run inference\n",
    "prompt = \"The meaning of life is \"\n",
    "res = llm(prompt, **generation_kwargs) # Res is a dictionary\n",
    "\n",
    "print(res[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdd7b41",
   "metadata": {},
   "source": [
    "# Speech To Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82b0c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# package installation\n",
    "# pip install SpeechRecognition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5fa48d11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "cead774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable - capture speech\n",
    "recognizer = speech.Recognizer()\n",
    "\n",
    "# global variable - conversation [end user question][machine response]\n",
    "convo = []\n",
    "\n",
    "# global variable - common sentences\n",
    "NOT_UNDERSTAND = \"Sorry, I didn't understand that. I only can understanding english\"\n",
    "ERROR = \"Unexpected Error Occurs. Message to Developer > \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "3e121fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to store conversation \n",
    "def setQuestion(ques): \n",
    "    convo.append([str(ques)])\n",
    "    \n",
    "# function to store conversation \n",
    "def setResponse(res):\n",
    "    convo[len(convo) - 1].append(str(res))\n",
    "\n",
    "## ( int - index , string )\n",
    "def setResponseWithNum(target, res):\n",
    "    convo[target].append(str(res))\n",
    "\n",
    "# Example of using\n",
    "# setQuestion(\"hello\")\n",
    "# setResponse(\"Hello how can i help u\")\n",
    "# [['hello', 'Hello how can i help u']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9b285be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to capture speech as input \n",
    "def capature_speech():\n",
    "    try:\n",
    "        with speech.Microphone() as mic:\n",
    "            print(\"listening\")\n",
    "            audio = recognizer.listen(mic, timeout=3)\n",
    "        return audio\n",
    "    except speech.WaitTimeoutError as e:\n",
    "        return None\n",
    "\n",
    "# ******** Error to be handle ******** \n",
    "# WaitTimeoutError: listening timed out while waiting for phrase to start\n",
    "# Define : no talking when listening "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee54e46",
   "metadata": {},
   "source": [
    "https://rollbar.com/blog/throwing-exceptions-in-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "844ca6b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "listening\n",
      "nihao\n"
     ]
    }
   ],
   "source": [
    "# converting speech to text\n",
    "def convert_speechToText(audio):\n",
    "    text = \"\"\n",
    "    try: \n",
    "        if audio != None:\n",
    "            # converting \n",
    "            text = recognizer.recognize_google(audio, language='en-US')\n",
    "    except speech.UnknownValueError:\n",
    "        # unknown language \n",
    "        text = NOT_UNDERSTAND # no speech / sound \n",
    "    except speech.RequestError as e:\n",
    "        text = ERROR.format(e)\n",
    "    return text # audio = null ; text = null\n",
    "\n",
    "# print(convert_speechToText(capature_speech()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e1f87fd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
