{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5797873",
   "metadata": {},
   "source": [
    "### Resource use \n",
    "\n",
    "- [Reference Video](https://www.youtube.com/watch?v=u2diEa4VT4M&t=83s&ab_channel=AllAboutAI)\n",
    "- [Run Llama 2 Locally with Python](https://swharden.com/blog/2023-07-29-ai-chat-locally-with-python/)\n",
    "- [llama-cpp-python](https://pypi.org/project/llama-cpp-python/)\n",
    "    - Tutorial \n",
    "        - https://www.datacamp.com/tutorial/llama-cpp-tutorial\n",
    "- [Mistral-7B-Instruct-v0.1-GGUF](https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.1-GGUF)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4f0027",
   "metadata": {},
   "source": [
    "## Model Installing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "441d7513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install llama-cpp-python\n",
    "\n",
    "# version check \n",
    "# pip show llama-cpp-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5597cce6",
   "metadata": {},
   "source": [
    "### Test Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709195d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the large language model file\n",
    "from llama_cpp import Llama\n",
    "LLM = Llama(model_path=\"model/mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "\n",
    "# create a text prompt\n",
    "prompt = \"Q: What are the names of the days of the week?\"\n",
    "\n",
    "# generate a response (takes several seconds)\n",
    "output = LLM(prompt)\n",
    "\n",
    "# display the response\n",
    "print(prompt)\n",
    "print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe82b0ee",
   "metadata": {},
   "source": [
    "## Train Model - Fine Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db34e62",
   "metadata": {},
   "source": [
    "[ShortCut Key](https://digitalhumanities.hkust.edu.hk/tutorials/jupyter-notebook-tips-and-shortcuts/)\n",
    "\n",
    "<br>\n",
    "https://www.activestate.com/resources/quick-reads/how-to-access-a-row-in-a-dataframe/\n",
    "\n",
    "<hr>\n",
    "\n",
    "### Reference \n",
    "- [Guide to Fine-Tuning LLMs](https://www.datacamp.com/tutorial/fine-tuning-large-language-models)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dca3d08",
   "metadata": {},
   "source": [
    "# Clean Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d8d411",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load model\n",
    "from llama_cpp import Llama\n",
    "LLM = Llama(model_path=\"model/mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "\n",
    "# open dataset \n",
    "business_json_path = business_json_path = 'dataset/yelp_academic_dataset_business.json'\n",
    "df_b = pd.read_json(business_json_path, lines=True)\n",
    "\n",
    "# convert json to csv\n",
    "csv_name = \"dataset/business.csv\"\n",
    "df_b.to_csv(csv_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3d0b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataset\n",
    "business = pd.read_csv(csv_name, index_col='business_id')\n",
    "\n",
    "display(business)\n",
    "display(business.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa56a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter dataset - only take categories include restaurant and food & isOpen = 1 \n",
    "# categories = pd.read_csv('business.csv', index_col='business_id')\n",
    "\n",
    "# check this business categories have food / restaurant \n",
    "def isRestaurant(categories: str):\n",
    "    if type(categories) == str:\n",
    "        categories = categories.lower()\n",
    "        if 'restaurants' in categories or 'food' in categories:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def isOpen(open: int):\n",
    "    if open == 1:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "row, col = business.shape\n",
    "\n",
    "index : int = []\n",
    "\n",
    "for i in range(len(business)):\n",
    "    if isRestaurant(business.iloc[i]['categories']) == False:\n",
    "        # print(categories.iloc[i]['categories'])\n",
    "        index.append(i)\n",
    "    elif isOpen(business.iloc[i]['is_open']) == False:\n",
    "        index.append(i)\n",
    "\n",
    "business.drop(business.index[index], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ed81676",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249f153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export clean data\n",
    "business.to_csv(\"dataset/business.csv\")\n",
    "print(business.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda5cb4",
   "metadata": {},
   "source": [
    "# Prepare Training Dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc68c33b",
   "metadata": {},
   "source": [
    "https://discuss.huggingface.co/t/from-pandas-dataframe-to-huggingface-dataset/9322/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea78a83d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Loading the dataset to train our model\n",
    "dataset = load_dataset(\"mteb/tweet_sentiment_extraction\")\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "print(dataset)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets['train'])\n",
    "display(tokenized_datasets['train'])\n",
    "\n",
    "display(pd.DataFrame.from_dict(tokenized_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dce983",
   "metadata": {},
   "outputs": [],
   "source": [
    "## -- above is reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee77791",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset into train and test\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import Dataset, DatasetDict\n",
    "import math\n",
    "\n",
    "dataframe = pd.read_csv(\"dataset/business.csv\", index_col='business_id')\n",
    "\n",
    "row, col = dataframe.shape\n",
    "# 70% of row become train data\n",
    "train_range = math.ceil(row * 0.7)\n",
    "\n",
    "tdf = dataframe.iloc[:train_range]\n",
    "vdf = dataframe.iloc[train_range:]\n",
    "tds = Dataset.from_pandas(tdf)\n",
    "vds = Dataset.from_pandas(vdf)\n",
    "\n",
    "\n",
    "dataset = DatasetDict()\n",
    "\n",
    "dataset['train'] = tds\n",
    "dataset['test'] = vds\n",
    "\n",
    "# print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df73bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to cannot using torch to train own model - at here using gpt2 as replace to do training\n",
    "from transformers import GPT2Tokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "def tokenize_function(examples):\n",
    "   return tokenizer(examples[\"business_id\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "print(tokenized_datasets)\n",
    "print(tokenized_datasets['train'])\n",
    "display(tokenized_datasets['train'])\n",
    "\n",
    "display(pd.DataFrame.from_dict(tokenized_datasets['train']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a82ca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acf8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2ForSequenceClassification\n",
    "\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\"gpt2\", num_labels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f71f576c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164642b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a68b8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"test_trainer\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    save_strategy=\"epoch\", \n",
    "    push_to_hub=True\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03613e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers[torch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27028abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504e41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install ctransformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab65f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "## dont runnnnnnnnnnnnnn\n",
    "\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "\n",
    "# Set gpu_layers to the number of layers to offload to GPU. Set to 0 if no GPU acceleration is available on your system.\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"TheBloke/Mistral-7B-Instruct-v0.1-GGUF\", model_file=\"mistral-7b-instruct-v0.1.Q4_K_M.gguf\", model_type=\"mistral\", gpu_layers=50)\n",
    "\n",
    "print(llm(\"AI is going to\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca260ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"model/mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"model/mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "\n",
    "text = \"\"\"<s>[INST] What is your favourite condiment? [/INST]\n",
    "Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!</s>\n",
    "[INST] Do you have mayonnaise recipes? [/INST]\"\"\"\n",
    "\n",
    "encodeds = tokenizer(text, return_tensors=\"pt\", add_special_tokens=False)\n",
    "\n",
    "model_inputs = encodeds.to(device)\n",
    "model.to(device)\n",
    "\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=1000, do_sample=True)\n",
    "decoded = tokenizer.batch_decode(generated_ids)\n",
    "print(decoded[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9822c30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip show torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3339256b",
   "metadata": {},
   "source": [
    "### Read data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3920c488",
   "metadata": {},
   "source": [
    "Error facing when installing trl, peft & torch\n",
    "\n",
    "ERROR: Cannot uninstall 'TBB'. It is a distutils installed project and thus\n",
    "    we cannot accurately determine which files belong to it which would lead to\n",
    "    only a partial uninstall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0a18a",
   "metadata": {},
   "source": [
    "[Pandas Official DataFrame Tutorial](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html)\n",
    "<br><br>\n",
    "[W3School Pandas DataFrame](https://www.w3schools.com/python/pandas/pandas_dataframes.asp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efefeb7d",
   "metadata": {},
   "source": [
    "# Speech To Text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bf6612",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as speech\n",
    "\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff878ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variable - capture speech\n",
    "recognizer = speech.Recognizer()\n",
    "\n",
    "# global variable - conversation [end user question][machine response]\n",
    "convo = []\n",
    "\n",
    "# global variable - common sentences\n",
    "NOT_UNDERSTAND = \"Sorry, I didn't understand that. I only can understanding english\"\n",
    "ERROR = \"Unexpected Error Occurs. Message to Developer > \"\n",
    "NOT_CLEAR = \"I'm sorry I didn't catch what you said. Could you repeat it, please\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f963c831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to store conversation \n",
    "def inRange(index):\n",
    "    return index >= 0 and index < len(convo) # true of false \n",
    "\n",
    "# setter \n",
    "def setQuestion(ques): \n",
    "    convo.append([str(ques)])\n",
    "    \n",
    "# function to store conversation \n",
    "def setResponse(res):\n",
    "    convo[len(convo) - 1].append(str(res))\n",
    "\n",
    "## ( int - index , string )\n",
    "def setResponseWithNum(index, res):\n",
    "    if inRange(index):\n",
    "        convo[index].append(str(res))\n",
    "        return True\n",
    "    return False\n",
    "    \n",
    "## ( int - index , string )\n",
    "def setResponseWithNum(index, res):\n",
    "    if inRange(index):\n",
    "        convo[index].append(str(res))\n",
    "        return True\n",
    "    return False \n",
    "\n",
    "# getter \n",
    "def getLatestQuestion():\n",
    "    return convo[len(convo) - 1][0]    \n",
    "\n",
    "def getLatestResponse():\n",
    "    return convo[len(convo) - 1][1]\n",
    "\n",
    "def getQuestionWithNum(index):\n",
    "    if inRange(index):\n",
    "        return convo[index][0]\n",
    "    # out of index\n",
    "    return None \n",
    "\n",
    "def getResponseWithNum(index):\n",
    "    if inRange(index):\n",
    "        return convo[index][1]\n",
    "    # out of index\n",
    "    return None \n",
    "\n",
    "# Example of using\n",
    "# setQuestion(\"hello\")\n",
    "# setResponse(\"Hello how can i help u\")\n",
    "# [['hello', 'Hello how can i help u']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477e748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to capture speech as input \n",
    "def capature_speech():\n",
    "    try:\n",
    "        with speech.Microphone() as mic:\n",
    "            print(\"listening\")\n",
    "            audio = recognizer.listen(mic, timeout=3)\n",
    "        return audio\n",
    "    except speech.WaitTimeoutError as e:\n",
    "        # within the time limit doesnot have any sound \n",
    "        raise speech.WaitTimeoutError(e)\n",
    "\n",
    "# ******** Error to be handle ******** \n",
    "# WaitTimeoutError: listening timed out while waiting for phrase to start\n",
    "# Define : no talking when listening "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fd2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting speech to text\n",
    "def convert_speechToText(audio):\n",
    "    text = \"\"\n",
    "    try: \n",
    "        # converting \n",
    "        text = recognizer.recognize_google(audio)\n",
    "        \n",
    "        return text\n",
    "    except speech.UnknownValueError as e:\n",
    "        # unknown language / no speech / sound  \n",
    "        raise speech.UnknownValueError(e)\n",
    "    except speech.RequestError as e:\n",
    "        raise speech.RequestError(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448dd3b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the large language model file\n",
    "def load_model():\n",
    "    return Llama(model_path=\"model/mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "\n",
    "LLM = load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84d92b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# only when the audio convert to text successful \n",
    "def generate_output(input):\n",
    "    # generate a response (takes several seconds)\n",
    "    output = LLM(input)\n",
    "    text = output[\"choices\"][0][\"text\"].strip()\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e5727b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def receive_inputVoice(): \n",
    "    text = \"\"\n",
    "    \n",
    "    try:\n",
    "        # create a text prompt\n",
    "        audio = capature_speech()\n",
    "        text = convert_speechToText(audio)\n",
    "\n",
    "        # store question \n",
    "        setQuestion(text)    \n",
    "        \n",
    "        return True\n",
    "\n",
    "    except speech.WaitTimeoutError as e:\n",
    "        # not sound when listening cause timeout\n",
    "        setQuestion(\"Problem occurs when receiving input\")\n",
    "        setResponse(NOT_CLEAR)\n",
    "    except speech.UnknownValueError as e:\n",
    "        # cause when convert problem\n",
    "        setQuestion(\"Problem occurs when receiving input\")\n",
    "        setResponse(NOT_UNDERSTAND)\n",
    "    except speech.RequestError as e:\n",
    "        # IDK\n",
    "        setQuestion(\"Problem occurs when receiving input\")\n",
    "        setResponse(ERROR + format(e))\n",
    "    except Exception as e: \n",
    "        setQuestion(\"Problem occurs when receiving input\")\n",
    "        setResponse(format(e))\n",
    "    return False # input get unsuccessful - no continuos processing \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c614921",
   "metadata": {},
   "source": [
    "# Business suggest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a52a6f72",
   "metadata": {},
   "source": [
    "- [The best post that I have ever seen](https://stackoverflow.com/questions/65199011/is-there-a-way-to-check-similarity-between-two-full-sentences-in-python)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f676800c",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/python-word-similarity-using-spacy/\n",
    "\n",
    "https://huggingface.co/docs/transformers/model_doc/bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8711f6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Setup\n",
    "# pip install spacy\n",
    "\n",
    "# python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bca97e05",
   "metadata": {},
   "source": [
    "## Check Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6174f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "def get_similarity(sent1, sent2):\n",
    "    doc1 = nlp(sent1)\n",
    "    doc2 = nlp(sent2)\n",
    "    print(doc1.similarity(doc2))\n",
    "    return doc1.similarity(doc2)\n",
    "\n",
    "def isSimilar(sent1, sent2):\n",
    "    if get_similarity(sent1.lower(), sent2.lower()) > 0.70:\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "# print(get_similarity('can you suggest a few restaurant in', 'please help me to found a restaurant in Malaysia, that have high rating'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d515bc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rule based for Hi and Hello\n",
    "\n",
    "def checkRegard(input):\n",
    "    if isSimilar(rule[0][0], input):\n",
    "        setResponse(rule[0][1])\n",
    "        return True\n",
    "    elif isSimilar(rule[1][0], input):\n",
    "        setResponse(rule[1][1])\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ec035d",
   "metadata": {},
   "source": [
    "## Convert convo into PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ec2775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install fpdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135f55fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from fpdf import FPDF\n",
    "import datetime as date\n",
    "\n",
    "# save FPDF() class into a \n",
    "# variable pdf\n",
    "pdf = FPDF()\n",
    " \n",
    "# Add a page\n",
    "pdf.add_page()\n",
    "\n",
    "# set style and size of font \n",
    "# that you want in the pdf\n",
    "pdf.set_font(\"Arial\", size = 15)\n",
    "\n",
    "for qna in convo:\n",
    "    pdf.cell(200, 10, txt = \"You : \" + qna[0][0], \n",
    "         ln = 1, align = 'L')\n",
    "    pdf.cell(200, 10, txt = \"Robot : \" + qna[0][1], \n",
    "         ln = 1, align = 'L')\n",
    "\n",
    "# save the pdf with name .pdf\n",
    "date = date.datetime.now()\n",
    "pdf.output(f\"convo_{date.year}{date.month}{date.day}{date.hour}{date.second}{date.microsecond}.pdf\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158ee70",
   "metadata": {},
   "source": [
    "# Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4392364",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyttsx3 # sound output "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee64f72",
   "metadata": {},
   "source": [
    "https://hackernoon.com/an-essential-python-text-to-speech-tutorial-using-the-pyttsx3-library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815dab7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text-to-speech setting \n",
    "import pyttsx3 as tts\n",
    "\n",
    "def machineInitSetting():\n",
    "    volume = machine.getProperty('volume')\n",
    "    machine.setProperty('volume', volume+1.00)\n",
    "    voices = machine.getProperty('voices')\n",
    "    machine.setProperty('voice', voices[0].id)\n",
    "\n",
    "def generate_sound(res):\n",
    "    machine.say(res)\n",
    "    machine.runAndWait()\n",
    "\n",
    "machine = tts.init()\n",
    "machineInitSetting()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b9591",
   "metadata": {},
   "source": [
    "## OpenVoice Problem\n",
    "- cannot install torch\n",
    "- https://www.youtube.com/watch?v=1ec-jOlxt_E&ab_channel=WingnutLabs\n",
    "- https://www.youtube.com/watch?v=dLNN36hU06M&ab_channel=MG\n",
    "- https://blog.unrealspeech.com/openvoice-completed-guide/\n",
    "- https://github.com/myshell-ai/OpenVoice/blob/main/demo_part1.ipynb\n",
    "- https://github.com/myshell-ai/OpenVoice/issues/98"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b479f2fc",
   "metadata": {},
   "source": [
    "# Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f0e8f0",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/converting-yelp-dataset-to-csv-using-pandas-2a4c8f03bd88 \n",
    "\n",
    "### Convert json to csv\n",
    "https://www.squash.io/how-to-convert-json-to-csv-in-python/\n",
    "\n",
    "https://www.w3schools.com/python/pandas/pandas_json.asp\n",
    "\n",
    "https://www.analyticsvidhya.com/blog/2021/06/data-cleaning-using-pandas/\n",
    "\n",
    "https://towardsdatascience.com/yelp-restaurant-recommendation-system-capstone-project-264fe7a7dea1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1f935b",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a05451",
   "metadata": {},
   "source": [
    "https://youtu.be/CkkjXTER2KE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b004c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from difflib import get_close_matches\n",
    "\n",
    "def load_knowledge_base(file_path: str) -> dict:\n",
    "    with open(file_path, 'r') as file:\n",
    "        data: dict = json.load(file)\n",
    "    return data\n",
    "\n",
    "\n",
    "def save_knowledge_base(file_path: str, data: dict):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "\n",
    "\n",
    "def find_best_match(user_question: str, questions: list[str]) -> str | None:\n",
    "    # 60 % similiar res\n",
    "    matches: list = get_close_matches(user_question, questions, n=1, cutoff=0.6)\n",
    "    return matches[0] if matches else None\n",
    "\n",
    "\n",
    "\n",
    "def get_answer_for_question(question: str, knowledge_base: dict) -> str | None:\n",
    "    for q in knowledge_base[\"questions\"]:\n",
    "        if q[\"question\"] == question:\n",
    "            return q[\"answer\"]\n",
    "\n",
    "def chat_bot():\n",
    "    knowledge_base: dict = load_knowledge_base('knowledge_base.json')\n",
    "        \n",
    "    while True:\n",
    "        user_input: str = input('You : ')\n",
    "            \n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # search best match inside json file \n",
    "        best_match: str | None = find_best_match(user_input, [q['question'] for q in knowledge_base['questions']])\n",
    "        \n",
    "        if best_match:\n",
    "            answer: str = get_answer_for_question(best_match, knowledge_base)\n",
    "            print(f'Bot : {answer}')\n",
    "            generate_sound(answer)\n",
    "        else:\n",
    "            # let machine to make a response \n",
    "            answer = generate_output(user_input)\n",
    "            print(f'Bot : {answer}')\n",
    "            generate_sound(answer)\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    chat_bot()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2ba276",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "\n",
    "https://www.youtube.com/watch?v=wCuJncQsXxI&ab_channel=Anotherbarefooteel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39996e01",
   "metadata": {},
   "source": [
    "## Keyword extract\n",
    "\n",
    "https://stackoverflow.com/questions/27405942/best-way-to-extract-keywords-from-input-nlp-sentence\n",
    "\n",
    "https://stackoverflow.com/questions/3788870/how-to-check-if-a-word-is-an-english-word-with-python\n",
    "\n",
    "### get Location\n",
    "https://stackoverflow.com/questions/56655312/retrieving-full-address-and-geocoding-based-on-place-store-name-and-city-stored\n",
    "\n",
    "https://stackoverflow.com/questions/49518172/how-to-find-place-name-inside-a-sentence-using-nlp-and-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82edbc0a",
   "metadata": {},
   "source": [
    "## Fine Tuning\n",
    "\n",
    "https://www.datacamp.com/tutorial/fine-tuning-large-language-models\n",
    "\n",
    "https://huggingface.co/docs/datasets/en/loading\n",
    "\n",
    "https://www.datacamp.com/tutorial/mistral-7b-tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dedcf7",
   "metadata": {},
   "source": [
    "#### PyAudio\n",
    "https://xn--llions-yua.jutge.org/upc-python-cookbook/signal-processing/audio-image.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86c2447",
   "metadata": {},
   "source": [
    "# Algorithm global setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ques = \"Happy Birthday\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ccd912",
   "metadata": {},
   "source": [
    "## Algorithm 1 - GPT2\n",
    "\n",
    "https://huggingface.co/docs/transformers/en/model_doc/gpt2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639bb89b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "def get_res_from_gpt2(ques: str):\n",
    "    model_gpt2 = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "    tokenizer_gpt2 = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "    input_ids = tokenizer(ques, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    gpt2_tokens = model_gpt2.generate(\n",
    "        input_ids,\n",
    "        do_sample=True,\n",
    "        temperature=0.9,\n",
    "        max_length=100,\n",
    "    )\n",
    "\n",
    "    gpt2_res = tokenizer.batch_decode(gpt2_tokens)[0]\n",
    "    \n",
    "    return gpt2_res\n",
    "\n",
    "# print(type(gpt2_res))\n",
    "# print(ques)\n",
    "# print(gpt2_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dac4080",
   "metadata": {},
   "source": [
    "## Algorithm 2 - Mistral 7b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac2fddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the large language model file\n",
    "from llama_cpp import Llama\n",
    "\n",
    "def get_res_from_mistral(ques: str):\n",
    "    LLM = Llama(model_path=\"model/mistral-7b-instruct-v0.1.Q5_K_M.gguf\")\n",
    "    \n",
    "    # generate a response (takes several seconds)\n",
    "    output = LLM(ques)\n",
    "    \n",
    "    return output[\"choices\"][0][\"text\"]\n",
    "\n",
    "# display the response\n",
    "# print(ques)\n",
    "# print(output[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d86306",
   "metadata": {},
   "source": [
    "## Algorithm 3 - DialoGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff90e97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "def get_res_from_dialo(ques: str):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(ques + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    bot_input_ids = new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "    \n",
    "    # pretty print last ouput tokens from bot\n",
    "    return format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True))\n",
    "\n",
    "# print(chat_history_ids)\n",
    "# print(chat_history_ids[:, bot_input_ids.shape[-1]:][0])\n",
    "# print(bot_input_ids.shape[-1])\n",
    "# print(bot_input_ids.shape)\n",
    "\n",
    "\n",
    "# print(get_res_from_dialo(ques))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b04e659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "# import torch\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "# model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-medium\")\n",
    "\n",
    "# # Let's chat for 5 lines\n",
    "# for step in range(5):\n",
    "#     # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "#     new_user_input_ids = tokenizer.encode(input(\">> User : \") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "#     # append the new user input tokens to the chat history\n",
    "#     bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "#     # generated a response while limiting the total chat history to 1000 tokens, \n",
    "#     chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "#     # pretty print last ouput tokens from bot\n",
    "#     print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a36665",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
